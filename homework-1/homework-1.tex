\documentclass[12pt]{article}

\usepackage[margin=0.75in, top=1in, bottom=1in, a4paper]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{indentfirst}

\newcommand{\mo}[1]{\lvert #1 \rvert}
\newcommand{\mos}[1]{\lvert #1 \rvert^2}
\newcommand{\mov}[1]{\lvert \vec{#1} \rvert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\p}{\partial}
\newcommand{\iv}[1]{\langle #1 \rangle}
\newcommand{\adj}{\text{adj}}
\newcommand{\pmax}{\text{pmax}}
\newcommand{\dom}{\text{dom}}
\newcommand{\conv}{\text{conv}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\theoremstyle{definition}
\newtheorem{prop}{Proposition}[section]
\newtheorem{prob}[prop]{Problem}
\newtheorem{ex}{Exercise}

\title{\vspace{-2.0cm}Convex Optimization Homework 1}
\author{Linxuan Ma}

\begin{document}
	\maketitle
	
	\section{Convex Sets}
	
	\begin{prop}
		A polyhedron $\{x \in \RR^n : Ax \leq b\}$ for $A \in \RR^{m \times n}$ and $b \in \RR^m$ is both convex and closed.
	\end{prop}
	\begin{proof}
		The polyhedron $P = \{x \in \RR^n : Ax \leq b\}$ can be described as the intersection of $n(P)$ half-spaces, denoted by set $H$:
		\begin{gather*}
			A_{1,1}x_1 + A_{1,2}x_2 + \dots + A_{1,n}x_n \leq  b_1 \\
			\dots \\
			A_{m,1}x_1 + A_{m,2}x_2 + \dots + A_{m,n}x_n \leq  b_m
		\end{gather*}
		
		$\forall a, b \in P$, both $a$ and $b$ are in all of the above half-spaces (by definition of intersection $P \in H_i$ for all $i$), and the line connecting them can be obtained via their convex combination $z(t) = tx + (1-t)y$ for $t \in [0, 1]$. Half-spaces are convex, and therefore for any half-space $H$, $x, y \in H \Rightarrow z(t) \in H$. Since the above is satisfied for all above half-spaces,  $z(t) \in P$ (definition of intersection). Therefore, $P$ satisfies that $x, y \in P \Rightarrow tx + (1-t)y \in P$, proving that $P$ is indeed convex.
		
		The closeness of $P$ can be proven by considering the open set complement $\overline{H_i}$ of every half-space $H_i$. $\bigcup_i \overline{H_i}$ is an open set, and $P = \bigcup_i H_i$ is its complement. Therefore $P$ is a closed set (definition of closeness).
	\end{proof}
	
	\begin{prop}
		Given a set of convex sets $S$, the intersection $\bigcap_i S_i$ is convex. Similarly, given a set of closed sets $S$, the intersection $\bigcap_i S_i$ is closed.
	\end{prop}
	
	\begin{proof}
		The intersection $P = \bigcap_i S_i$ is a subset of every $S_i$ (definition of intersection). For all $x, y \in P, t \in [0, 1]$, $tx + (1-t)y \in S_i$ for all $i$ (definition of convex set). The line $tx + (1-t)y$ is contained in every $S_i$, therefore it is contained in the intersection $P$ of all $S_i$ (definition of intersection).
		
		(Same as \textit{1.1}) The closeness of $P$ can be proven by considering the open set complement $\overline{H_i}$ of every half-space $H_i$. $\bigcup_i \overline{H_i}$ is an open set, and $P = \bigcup_i H_i$ is its complement. Therefore $P$ is a closed set (definition of closeness).
	\end{proof}
	
	\begin{prob}
		Give an example of a closed set in $\RR^2$ whose convex set is not closed.
	\end{prob}
	
	\begin{proof}
		The set of $(x, y)$ satisfying
		\begin{gather*}
			y \geq \frac{1}{x^2 + c}
		\end{gather*}
		where $c \in \RR_+$ is a closed set, while its convex hull is any $(x_c, y_c)$ satisfying $y > 0$ (an open set).
	\end{proof}
	
	\begin{prop}
		Consider matrix $A \in \RR^{m \times n}$. If set $S \subseteq \RR^m$ is convex, then its pre-image of $S$ under $A$ is also convex. The same applies for closeness.
	\end{prop}
	
	\begin{proof}
		Let $P$ be the pre-image of $S$ under $A$. Consider points $x, y \in P$ and the image of their convex combination under $A$:
		\begin{align*}
			&A(tx + (1 - t)y) \\
			\iff &A(tx) + A((1 - t)y) \\
			\iff &t*Ax + (1-t) * Ay
		\end{align*}
		
		$t*Ax + (1-t) * Ay \in S$ (definition of convexity) as $Ax$ and $Ay$ are both contained in $S$ (definition of pre-image). Therefore, $A(tx + (1 - t)y) \in S \Rightarrow tx + (1 - t)y \in P$.
	\end{proof}
	
	\begin{prop}
		Consider matrix $A \in \RR^{m \times n}$. If set $S \subseteq \RR^n$ is convex, then its image under $A$ is also convex.
	\end{prop}
	
	\begin{proof}
		Consider points $x, y \in A(S)$ and some points $x_p, y_p \in S$ such that $A(x_p) = x$, $A(y_p) = y$. Since $S$ is convex, then the convex combination of $x$ and $y$ can be is equivalent to:
		\begin{align*}
			&tx + (1 - t)y \\
			\iff &t * A(x_p) + (1 - t) * A(y_p) \\
			\iff &A(tx_p) + A((1 - t)y_p) \\
			\iff &A(tx_p + (1 - t)y_p)
		\end{align*}
		
		Since $S$ is convex, $tx_p + (1 - t)y_p \in S$. Therefore, $A(tx_p + (1 - t)y_p) \in A(S)$, and equivalently, $tx + (1 - t)y \in A(S)$.
	\end{proof}
	
	\begin{prop}
		Given an example of a matrix $A \in \RR^{m \times n}$ and a set $S \subseteq \RR^n$ that is closed and convex but such that $A(S)$ is not closed.
	\end{prop}
	
	\begin{proof}
		Consider the matrix $A: \RR^2 \to \RR^2$ that discards the $x$-component:
		\begin{equation*}
		\begin{bmatrix}
			0&0\\0&1	
		\end{bmatrix}
		\end{equation*}
		
		The set $S = \{(x, y):y \geq 0.5^x\}$ is both closed and convex, yet its image under $A$ is $0 \times (0, \infty)$, which is not closed.
	\end{proof}
	
	\begin{prop}
		Given polyhedron $P \subseteq \RR^n$, its image $A(P)$ under $A \in \RR^{m \times n}$ is a polyhedron.
	\end{prop}
	
	\begin{proof}
		Consider set $O = \{(x, y): Cx \leq c, y=Ax\}$ for $x \in P$ and  some arbitrary constraint of matrix $C$ and constant $c$. $O$ is polyhedral as it can be rewritten as a set of linear (in)equalities:
		\begin{align*}
			&O = \{(x, y): Cx \leq c, y=Ax\}\\
			\iff &O = \{(x, y): Cx \leq c, y \leq Ax, y \geq Ax\} \\
			\iff &O = \{(x, y): Cx \leq c, y - Ax\leq 0, Ax - y \leq 0\}
		\end{align*}
		
		The projection $f: (x, y) \to y$ preserves polyhedron, as indicated by the fact that (provided in proposition):
		\begin{gather*}
			P \subseteq \RR^{m + n} \text{ is a polyhedron} \Rightarrow \{x \in \RR^n: (x, y) \in P\} \text{ is a polyhedron}
		\end{gather*}
		
		Since $O$ is polyhedral, $f(O)$ preserves polyhedron and yields $P$. Therefore $P$ is a polyhedron.
	\end{proof}
	
	\begin{prop}
		If $Q \subseteq \RR^m$ is a polyhedron, then its pre-image under $A \in \RR^{m \times n}$ is a polyhedron.
	\end{prop}
	
	\begin{proof}
		WIP.
	\end{proof}
	
	\section{Convex Functions}
	
	\begin{prop}
		The entropy function, defined as:
		\begin{gather*}
			f(x) = -\sum^n_{i=1} x_i \log(x_i)
		\end{gather*}
		is strictly concave over the domain $\{x \in \RR^n_{++}: \sum^n_{i=1}x_i = 1\}$
	\end{prop}
	
	\begin{proof}
		The derivative of $f(x) = -u\log(x)$ is $-(1 + \log(x))$, which is strictly decreasing, thus strict concavity of $f$. The sum of a strictly concave function with another concave function is strictly concave, thus strict concavity of the entropy function.
	\end{proof}
	
	\begin{prop}
		Consider twice-differentiable function $f$ over a convex domain. The following statements are equivalent:
		\begin{enumerate}
			\item $f$ is convex.
			\item The gradient $\nabla f$ is monotone: $(\nabla f(x) - \nabla f(y))^\top (x - y) \geq 0$.
		\end{enumerate}
	\end{prop}
	
	\begin{proof}
		If $f$ is a convex function, then its gradient must be non-decreasing, and thus for any $x, y \in \dom(f)$:
		\begin{align*}
			f(y) &\geq f(x) + \nabla f(x)^\top (y - x) \\
			f(x) &\geq f(y) + \nabla f(y)^\top (x - y)
		\end{align*}
		
		Adding the LHS and RHS shows that convexity implies gradient monotonicity:
		\begin{align*}
			&f(x) + f(y) \geq f(x) + f(y) + \nabla f(x)^\top (y - x) + \nabla f(y)^\top (x - y) \\
			\iff &f(x) + f(y) - (f(x) + f(y)) \geq -\nabla f(x)^\top (x - y) + \nabla f(y)^\top (x - y) \\
			\iff &0 \geq (\nabla f(y) - \nabla f(x))^\top (x - y) \\
			\iff &0 \leq (\nabla f(x) - \nabla f(y))^\top (x - y)
		\end{align*}
		
		For the reverse, consider the interpolation function $l(t) = f(x + t * (y - x))$ for any $x, y \in \dom(f)$ and $t \in [0, 1]$. Its derivative with respect to $t$ is $l'(t) = \nabla f(x + t * (y - x))^\top (y - x)$. Observe that $l'(t) \geq l'(0)$ for any $t$ as $\nabla f$ is monotone. Therefore:
		\begin{align*}
			&\int^1_0 l'(t) \diff t \geq l'(0) \\
			\Rightarrow &l(0) + \int^1_0 l'(t) \diff t \geq l(0) + l'(0) \\
			\Rightarrow &l(1) \geq l(0) + l'(0)
		\end{align*}
		
		 Since $f(x) = l(0)$ and $f(y) = l(1)$:
		 \begin{gather*}
		 	l(1) \geq l(0) + l'(0) \iff f(y) \geq \nabla f(x)^\top (y - x)
		 \end{gather*}
		 
		 Therefore $f$ is convex.
	\end{proof}
	
	\begin{prop}
		Given an example of a strictly convex function that does not  attain its infimum.
	\end{prop}
	
	\begin{proof}
		The function:
		\begin{gather*}
			f(x) = \frac{1}{x}
		\end{gather*}
		is strictly convex, as its derivative, $f'(x) = -\frac{1}{x^2}$ is strictly increasing. $f$ does not attain its infimum at $0$ as its range is $(0, \infty)$.
	\end{proof}
	
	\begin{prop}
		A twice differentiable, strongly convex function is coercive and hence attain its infimum.
	\end{prop}
	
	\begin{proof}
		WIP.
	\end{proof}
	
	\begin{prop}
		The maximum of a convex function over a bounded polyhedron must occur at one of the vertices.
	\end{prop}
	
	\begin{proof}
		Consider vertices $V = \{v_1, \dots, v_n\}$ and the polyhedron $P = \conv(V)$ that $V$ forms. All points in $P$ can be written as some convex combination of $V$ for some $\lambda$: $\sum^n_{i=1} \lambda_i v_i$ given that $\sum^n_{i = 1} \lambda_i = 1$ and $\lambda_i \geq 0$. Due to convexity of $f$:
		\begin{gather*}
			f(\sum^n_{i=1} \lambda_i v_i) \leq \sum^n_{i=1} \lambda_i f(v_i)
		\end{gather*}
		
		Since $\lambda_i \leq 1$ for all $i$:
		\begin{gather*}
			\sum^n_{i=1} \lambda_i f(v_i) \leq \max_i f(v_i)
		\end{gather*}
		
		Therefore the maximum of $f$ over $P$ occurs at one of its vertices.
	\end{proof}
	
	\section{Partial Optimization with $\ell_2$ Penalties}
	
	Consider the problem
	\begin{gather*}
		\min_{\beta,\ \sigma \geq 0} f(\beta) + \frac{\lambda}{2}\sum_{i=1}^{n} g(\beta_i, \sigma_i)
	\end{gather*}
	for convex function $f: \RR^n \to \RR$, $\lambda \geq 0$, and
	\[
	g(x, y) =
	\begin{cases}
		x^2/y + y & y > 0 \\
		0 & x = 0,\ y = 0 \\
		\infty & \text{otherwise}
	\end{cases}
	\]
	
	\begin{prop}
		Proof that $g$ is convex, and that the above problem is a convex problem.
	\end{prop}
	
	\begin{proof}
		WIP.
	\end{proof}
	
	\begin{prop}
		Proof that $\min_{y \geq 0} g(x, y) = 2 \mo{x}$.
	\end{prop}
	
	\begin{proof}
		By fixing $x$ and considering $y \geq 0$, the minimum on $y$ is obtained via:
		\begin{gather*}
			\frac{\partial g}{\partial y} = 0 \implies -\frac{x^2}{y^2} + 1 = 0 \implies  y^2 = x^2 \implies y = \mo{x}
		\end{gather*}
		
		Plugging into $g$:
		\begin{gather*}
			\frac{x^2}{\mo{x}} + \mo{x} \implies \mo{x} + \mo{x} = 2\mo{x}
		\end{gather*}
	\end{proof}
	
\end{document}
